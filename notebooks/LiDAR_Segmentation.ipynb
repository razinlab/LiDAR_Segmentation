{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LiDAR Segmentation\n",
        "---\n",
        "This will be a notebook focusing on the LiDAR specific portions of the overall project.\n",
        "\n",
        "First, let's start with a spherical projection to convert the 3D point clouds into a 2D projection. We will write this in Python/NumPy and then compare to a GPU accelerated version."
      ],
      "metadata": {
        "id": "g9n5f7LLY87p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9PZfVkyYr81",
        "outputId": "292fa957-de7f-4ccd-c68e-c4aff13a9027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/\")"
      ],
      "metadata": {
        "id": "Llnv-zkTgUvq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cnn\n",
        "from cnn import MultiClassCrossEntropyLoss, Adam\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "k9L5VcezgvXW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOWER_FOV = np.deg2rad(-25)\n",
        "UPPER_FOV = np.deg2rad(25)\n",
        "VERTICAL_FOV = UPPER_FOV - LOWER_FOV\n",
        "\n",
        "def spherical_projection(points, intensities, image_height, image_width):\n",
        "  epsilon = 1e-6\n",
        "  r = np.sqrt(points[:, 0]**2 + points[:, 1]**2 + points[:, 2]**2)\n",
        "  r_safe = np.maximum(r, epsilon)\n",
        "\n",
        "  theta = np.arctan2(points[:, 1], points[:, 0])\n",
        "  phi = np.arcsin(points[:, 2] / r_safe)\n",
        "\n",
        "  u = ((theta + np.pi) / (2 * np.pi)) * image_width\n",
        "  v = 1 - (((phi - LOWER_FOV) / VERTICAL_FOV) * image_height)\n",
        "\n",
        "  # Prevent image overflow\n",
        "  u = np.clip(u, 0, image_width - 1)\n",
        "  v = np.clip(v, 0, image_height - 1)\n",
        "\n",
        "  u = u.astype(np.int32)\n",
        "  v = v.astype(np.int32)\n",
        "\n",
        "  projection_image = np.zeros((image_height, image_width, 5), dtype=np.float32)\n",
        "\n",
        "  valid_indices_mask = (u >= 0) & (u < image_width) & (v >= 0) & (v < image_height)\n",
        "  original_indices = np.where(valid_indices_mask)[0]\n",
        "\n",
        "  u_valid = u[valid_indices_mask]\n",
        "  v_valid = v[valid_indices_mask]\n",
        "  r_valid = r[valid_indices_mask]\n",
        "  points_valid = points[valid_indices_mask]\n",
        "  intensities_valid = intensities[valid_indices_mask]\n",
        "\n",
        "  # Since some points might map to the same pixel, we will keep the closest one\n",
        "  sort_indices = np.argsort(r_valid)\n",
        "\n",
        "  u_sorted = u_valid[sort_indices]\n",
        "  v_sorted = v_valid[sort_indices]\n",
        "  original_indices_sorted = original_indices[sort_indices]\n",
        "\n",
        "  pixel_ids = v_sorted * image_width + u_sorted\n",
        "  _, unique_indices = np.unique(pixel_ids, return_index=True)\n",
        "\n",
        "  final_indices_in_sorted = unique_indices\n",
        "  final_u = u_sorted[final_indices_in_sorted]\n",
        "  final_v = v_sorted[final_indices_in_sorted]\n",
        "  final_r = r_valid[sort_indices][final_indices_in_sorted]\n",
        "  final_points = points_valid[sort_indices][final_indices_in_sorted]\n",
        "  final_intensities = intensities_valid[sort_indices][final_indices_in_sorted]\n",
        "  final_original_indices = original_indices_sorted[final_indices_in_sorted]\n",
        "\n",
        "  final_data = np.column_stack((final_r, final_intensities, final_points))\n",
        "  projection_image[final_v, final_u] = final_data\n",
        "\n",
        "  return projection_image, final_v, final_u, final_original_indices"
      ],
      "metadata": {
        "id": "1SUv8WOZgxnk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder-Decoder Model\n",
        "---\n",
        "Now that we have all of the necessary pieces, we can construct the actual model for segmentation.\n",
        "\n",
        "We will be using a basic encoder-decoder model architecture."
      ],
      "metadata": {
        "id": "I67LK2yqJEJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic KITTI dataset uses 19 classes for evaluation and a 0 class for extraneous classes\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "model = cnn.Sequential([\n",
        "    # Encoder\n",
        "    cnn.Conv2D(in_channels=5, out_channels=32, kernel_size=3, padding=1),\n",
        "    cnn.ReLU(),\n",
        "    cnn.MaxPool2D(pool_size=2, stride=2),\n",
        "\n",
        "    cnn.Conv2D(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "    cnn.ReLU(),\n",
        "    cnn.MaxPool2D(pool_size=2, stride=2),\n",
        "\n",
        "    # Decoder\n",
        "    cnn.Upsample(scale_factor=2),\n",
        "    cnn.Conv2D(in_channels=64, out_channels=32, kernel_size=3, padding=1),\n",
        "    cnn.ReLU(),\n",
        "\n",
        "    cnn.Upsample(scale_factor=2),\n",
        "\n",
        "    # Prediction layer\n",
        "    cnn.Conv2D(in_channels=32, out_channels=NUM_CLASSES, kernel_size=1)\n",
        "])"
      ],
      "metadata": {
        "id": "TpY2UrAsJDoJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader\n",
        "---\n",
        "We need a way to feed in the LiDAR data into our model, for this we will construct a dataloader class"
      ],
      "metadata": {
        "id": "tUtj4VlPKudr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LidarDataset:\n",
        "  def __init__(self, zip_file_path, sequences):\n",
        "    self.zip_file = zipfile.ZipFile(zip_file_path, 'r')\n",
        "    all_files = self.zip_file.namelist()\n",
        "\n",
        "    self.point_cloud_files = []\n",
        "    self.label_files = []\n",
        "\n",
        "    for seq in sequences:\n",
        "      pc_paths = sorted([f for f in all_files if f.startswith(f'dataset/sequences/{seq}/voxels/') and f.endswith('.bin')])\n",
        "      label_paths = sorted([f for f in all_files if f.startswith(f'dataset/sequences/{seq}/voxels/') and f.endswith('.label')])\n",
        "\n",
        "      self.point_cloud_files.extend(pc_paths)\n",
        "      self.label_files.extend(label_paths)\n",
        "\n",
        "    # We need to remap the point cloud labels into a standardized format\n",
        "      self.label_remap = {\n",
        "      0: 0,    # \"unlabeled\"\n",
        "      1: 0,    # \"outlier\"\n",
        "      10: 1,   # \"car\"\n",
        "      11: 2,   # \"bicycle\"\n",
        "      13: 5,   # \"bus\"\n",
        "      15: 3,   # \"motorcycle\"\n",
        "      16: 5,   # \"on-rails\"\n",
        "      18: 4,   # \"truck\"\n",
        "      20: 5,   # \"other-vehicle\"\n",
        "      30: 6,   # \"person\"\n",
        "      31: 7,   # \"bicyclist\"\n",
        "      32: 8,   # \"motorcyclist\"\n",
        "      40: 9,   # \"road\"\n",
        "      44: 10,  # \"parking\"\n",
        "      48: 11,  # \"sidewalk\"\n",
        "      49: 12,  # \"other-ground\"\n",
        "      50: 13,  # \"building\"\n",
        "      51: 14,  # \"fence\"\n",
        "      52: 0,   # \"other-structure\"\n",
        "      60: 9,   # \"lane-marking\"\n",
        "      70: 15,  # \"vegetation\"\n",
        "      71: 16,  # \"trunk\"\n",
        "      72: 17,  # \"terrain\"\n",
        "      80: 18,  # \"pole\"\n",
        "      81: 19,  # \"traffic-sign\"\n",
        "      99: 0,   # \"other-object\"\n",
        "      252: 1,  # \"moving-car\"\n",
        "      253: 7,  # \"moving-bicyclist\"\n",
        "      254: 6,  # \"moving-person\"\n",
        "      255: 8,  # \"moving-motorcyclist\"\n",
        "      256: 5,  # \"moving-on-rails\"\n",
        "      257: 5,  # \"moving-bus\"\n",
        "      258: 4,  # \"moving-truck\"\n",
        "      259: 5   # \"moving-other-vehicle\"\n",
        "  }\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.point_cloud_files)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    pc_path = self.point_cloud_files[index]\n",
        "    label_path = self.label_files[index]\n",
        "\n",
        "    pc_bytes = self.zip_file.read(pc_path)\n",
        "    label_bytes = self.zip_file.read(label_path)\n",
        "\n",
        "    # Use np.frombuffer to convert the bytes into a NumPy array\n",
        "    point_cloud = np.frombuffer(pc_bytes, dtype=np.float32).reshape(-1, 4)\n",
        "    labels = np.frombuffer(label_bytes, dtype=np.uint32).reshape(-1)\n",
        "\n",
        "    points = point_cloud[:, :3]\n",
        "    intensities = point_cloud[:, 3]\n",
        "\n",
        "    image_height, image_width = 64, 1024\n",
        "    projected_image, v_coords, u_coords, kept_indices = spherical_projection(\n",
        "        points, intensities, image_height, image_width\n",
        "    )\n",
        "\n",
        "    projected_labels = np.zeros((image_height, image_width), dtype=np.int64)\n",
        "    kept_labels = labels[kept_indices]\n",
        "\n",
        "    remapped_labels = np.vectorize(lambda x: self.label_remap.get(x, 0))(kept_labels)\n",
        "    projected_labels[v_coords, u_coords] = remapped_labels\n",
        "\n",
        "    projected_image_transposed = projected_image.transpose(2, 0, 1).astype(np.float32)\n",
        "\n",
        "    return projected_image_transposed, projected_labels"
      ],
      "metadata": {
        "id": "rMagH8FOKtob"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "---\n",
        "Alright, our dataloder handles the retrieval of the point cloud data and also converts it into a projection for us\n",
        "\n",
        "Let's test out the model on some data, specifically the SemanticKITTI dataset.\n",
        "\n",
        "I have uploaded the zipped training dataset (3gb) into my Drive folder already.\n",
        "\n",
        "We will be streaming in the data batch by batch from the zipped folder as the entire dataset unzipped is about 100gb, which would be unmanageable."
      ],
      "metadata": {
        "id": "uCoX2MU7pkfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "---\n",
        "Now that we have all of our pieces, we can finally start training.\n"
      ],
      "metadata": {
        "id": "1nzHc0NEcN3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ZIP_PATH = '/content/drive/MyDrive/data_odometry_voxels_all.zip'\n",
        "TRAIN_SEQUENCES = ['00'] #, '01', '02', '03', '04', '05', '06', '07']\n",
        "VAL_SEQUENCES = ['08']\n",
        "\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "print(\"Initializing components...\")\n",
        "\n",
        "train_dataset = LidarDataset(zip_file_path=DATASET_ZIP_PATH, sequences=TRAIN_SEQUENCES)\n",
        "val_dataset = LidarDataset(zip_file_path=DATASET_ZIP_PATH, sequences=VAL_SEQUENCES)\n",
        "loss_fn = MultiClassCrossEntropyLoss()\n",
        "optimizer = Adam(lr=LEARNING_RATE, beta=0.9, gamma=0.999)\n",
        "print(f\"All components initialized. Training on {len(train_dataset) - 4521} samples.\")\n",
        "print(f\"All components initialized. Validating on {len(val_dataset) - 4051} samples.\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\n--- Starting Epoch {epoch+1}/{EPOCHS} ---\")\n",
        "\n",
        "    epoch_train_loss = 0.0\n",
        "    train_batch_count = 0\n",
        "    shuffled_indices = np.random.permutation(len(train_dataset))\n",
        "\n",
        "    x_batch, y_batch = [], []\n",
        "    for i in range(len(train_dataset) - 4521):\n",
        "        image, label = train_dataset[shuffled_indices[i]]\n",
        "        x_batch.append(image)\n",
        "        y_batch.append(label)\n",
        "\n",
        "        if len(x_batch) == BATCH_SIZE or i == len(train_dataset) - 1:\n",
        "            x_batch_np = np.array(x_batch)\n",
        "            y_batch_np = np.array(y_batch)\n",
        "\n",
        "            preds = model.forward(x_batch_np)\n",
        "            loss = loss_fn.forward(preds, y_batch_np)\n",
        "            grad = loss_fn.backward(preds, y_batch_np)\n",
        "            model.backward(grad, lambda_=0.0)\n",
        "            optimizer.step(model)\n",
        "\n",
        "            epoch_train_loss += loss\n",
        "            train_batch_count += 1\n",
        "            x_batch, y_batch = [], []\n",
        "\n",
        "    avg_train_loss = epoch_train_loss / train_batch_count\n",
        "\n",
        "    print(\"  Running validation...\")\n",
        "    epoch_val_loss = 0.0\n",
        "    val_batch_count = 0\n",
        "\n",
        "    x_val_batch, y_val_batch = [], []\n",
        "    for i in range(len(val_dataset) - 4051):\n",
        "        image, label = val_dataset[i]\n",
        "        x_val_batch.append(image)\n",
        "        y_val_batch.append(label)\n",
        "\n",
        "        if len(x_val_batch) == BATCH_SIZE or i == len(val_dataset) - 1:\n",
        "            x_val_batch_np = np.array(x_val_batch)\n",
        "            y_val_batch_np = np.array(y_val_batch)\n",
        "\n",
        "            # Get predictions and loss (No backward pass or optimizer step, it isn't necessary)\n",
        "            val_preds = model.forward(x_val_batch_np)\n",
        "            val_loss = loss_fn.forward(val_preds, y_val_batch_np)\n",
        "\n",
        "            epoch_val_loss += val_loss\n",
        "            val_batch_count += 1\n",
        "            x_val_batch, y_val_batch = [], []\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / val_batch_count\n",
        "    epoch_end_time = time.time()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_end_time - epoch_start_time:.2f}s\")\n",
        "\n",
        "print(\"\\n--- Training Finished ---\")"
      ],
      "metadata": {
        "id": "JUEeOdMrcY2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd66be6-f3c8-44d9-8d6d-5584079c35d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing components...\n",
            "All components initialized. Training on 20 samples.\n",
            "All components initialized. Validating on 20 samples.\n",
            "\n",
            "--- Starting Epoch 1/1 ---\n",
            "  Running validation...\n",
            "Epoch 1 Summary | Train Loss: 2.9727 | Val Loss: 2.9228 | Time: 98.96s\n",
            "\n",
            "--- Training Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The focus isn't accuracy here, just to get a baseline idea of performance in both training/inference. We can see training time on CPU for just 20 samples is about 99 seconds, which is extremely slow.\n",
        "\n",
        "Let's see how much faster GPU training is."
      ],
      "metadata": {
        "id": "YVAFueodDTHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Porting to PyTorch\n",
        "---\n",
        "Let's see how we can speed things up.\n",
        "\n",
        "We will convert the model architecture into PyTorch, as it takes care of GPU utilization for us and a custom CUDA kernel isn't necessary here."
      ],
      "metadata": {
        "id": "7Bir76Vj9Bsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LidarNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LidarNet, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Encoder\n",
        "            nn.Conv2d(in_channels=5, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Decoder\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Upsample(scale_factor=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=NUM_CLASSES, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = LidarNet()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4_a9akv9IPn",
        "outputId": "e2dc7f76-eb65-4d61-8aa8-34b35d51090f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LidarNet(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (7): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU()\n",
            "    (9): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (10): Conv2d(32, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also need to convert the dataloader as well as PyTorch handles dataloading slightly differently. The PyTorch model expects PyTorch tensors instead of arrays, this can be fixed by converting numpy arrays into tensors at the end but I will rewrite it here in case I need to use the different dataloaders and also for compatibility between PyTorch operations."
      ],
      "metadata": {
        "id": "BKK75p-zD57L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LidarDatasetPyTorch(Dataset): # Inherit from PyTorch's Dataset class\n",
        "    def __init__(self, zip_file_path, sequences):\n",
        "        self.zip_file = zipfile.ZipFile(zip_file_path, 'r')\n",
        "        all_files = self.zip_file.namelist()\n",
        "        self.point_cloud_files = []\n",
        "        self.label_files = []\n",
        "        for seq in sequences:\n",
        "            pc_paths = sorted([f for f in all_files if f.startswith(f'dataset/sequences/{seq}/voxels/') and f.endswith('.bin')])\n",
        "            label_paths = sorted([f for f in all_files if f.startswith(f'dataset/sequences/{seq}/voxels/') and f.endswith('.label')])\n",
        "            self.point_cloud_files.extend(pc_paths)\n",
        "            self.label_files.extend(label_paths)\n",
        "            self.label_remap = {\n",
        "                0: 0,    # \"unlabeled\"\n",
        "                1: 0,    # \"outlier\"\n",
        "                10: 1,   # \"car\"\n",
        "                11: 2,   # \"bicycle\"\n",
        "                13: 5,   # \"bus\"\n",
        "                15: 3,   # \"motorcycle\"\n",
        "                16: 5,   # \"on-rails\"\n",
        "                18: 4,   # \"truck\"\n",
        "                20: 5,   # \"other-vehicle\"\n",
        "                30: 6,   # \"person\"\n",
        "                31: 7,   # \"bicyclist\"\n",
        "                32: 8,   # \"motorcyclist\"\n",
        "                40: 9,   # \"road\"\n",
        "                44: 10,  # \"parking\"\n",
        "                48: 11,  # \"sidewalk\"\n",
        "                49: 12,  # \"other-ground\"\n",
        "                50: 13,  # \"building\"\n",
        "                51: 14,  # \"fence\"\n",
        "                52: 0,   # \"other-structure\"\n",
        "                60: 9,   # \"lane-marking\"\n",
        "                70: 15,  # \"vegetation\"\n",
        "                71: 16,  # \"trunk\"\n",
        "                72: 17,  # \"terrain\"\n",
        "                80: 18,  # \"pole\"\n",
        "                81: 19,  # \"traffic-sign\"\n",
        "                99: 0,   # \"other-object\"\n",
        "                252: 1,  # \"moving-car\"\n",
        "                253: 7,  # \"moving-bicyclist\"\n",
        "                254: 6,  # \"moving-person\"\n",
        "                255: 8,  # \"moving-motorcyclist\"\n",
        "                256: 5,  # \"moving-on-rails\"\n",
        "                257: 5,  # \"moving-bus\"\n",
        "                258: 4,  # \"moving-truck\"\n",
        "                259: 5   # \"moving-other-vehicle\"\n",
        "            }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.point_cloud_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pc_path = self.point_cloud_files[index]\n",
        "        label_path = self.label_files[index]\n",
        "        pc_bytes = self.zip_file.read(pc_path)\n",
        "        label_bytes = self.zip_file.read(label_path)\n",
        "        point_cloud = np.frombuffer(pc_bytes, dtype=np.float32).reshape(-1, 4)\n",
        "        labels = np.frombuffer(label_bytes, dtype=np.uint32).reshape(-1)\n",
        "        points = point_cloud[:, :3]\n",
        "        intensities = point_cloud[:, 3]\n",
        "        image_height, image_width = 64, 1024\n",
        "        projected_image, v_coords, u_coords, kept_indices = spherical_projection(\n",
        "            points, intensities, image_height, image_width\n",
        "        )\n",
        "        projected_labels = np.zeros((image_height, image_width), dtype=np.int64)\n",
        "        kept_labels = labels[kept_indices]\n",
        "        remapped_labels = np.vectorize(lambda x: self.label_remap.get(x, 0))(kept_labels)\n",
        "        projected_labels[v_coords, u_coords] = remapped_labels\n",
        "        projected_image_transposed = projected_image.transpose(2, 0, 1).astype(np.float32)\n",
        "\n",
        "        image_tensor = torch.from_numpy(projected_image_transposed)\n",
        "\n",
        "        label_tensor = torch.from_numpy(projected_labels)\n",
        "\n",
        "        return image_tensor, label_tensor"
      ],
      "metadata": {
        "id": "cjz87waxB7uS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "DATASET_ZIP_PATH = '/content/drive/MyDrive/data_odometry_voxels_all.zip'\n",
        "TRAIN_SEQUENCES = ['00']\n",
        "VAL_SEQUENCES = ['08']\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "train_dataset = LidarDatasetPyTorch(zip_file_path=DATASET_ZIP_PATH, sequences=TRAIN_SEQUENCES)\n",
        "val_dataset = LidarDatasetPyTorch(zip_file_path=DATASET_ZIP_PATH, sequences=VAL_SEQUENCES)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = LidarNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"Training on {len(train_dataset)} samples.\")\n",
        "print(f\"Validating on {len(val_dataset)} samples.\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\n--- Starting Epoch {epoch+1}/{EPOCHS} ---\")\n",
        "\n",
        "    model.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # Move data to the GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # 3. Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 4. Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(\"  Running validation...\")\n",
        "\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad(): # We don't need to calculate gradients for validation\n",
        "        for images, labels in val_loader:\n",
        "            # Move data to the GPU\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    epoch_end_time = time.time()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_end_time - epoch_start_time:.2f}s\")\n",
        "\n",
        "print(\"\\n--- Training Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDuwhmPPCTHl",
        "outputId": "0e9a2a79-966b-46cc-ebe6-e3ef191a2ab9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training on 4541 samples.\n",
            "Validating on 4071 samples.\n",
            "\n",
            "--- Starting Epoch 1/1 ---\n",
            "  Running validation...\n",
            "Epoch 1/1 | Train Loss: 2.1651 | Val Loss: 1.7500 | Time: 147.65s\n",
            "\n",
            "--- Training Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profiling\n",
        "---\n",
        "Let's conduct a thorough profile of the system to see where it is stalling,  although it will be pretty obvious where it is (spherical projection).\n",
        "\n"
      ],
      "metadata": {
        "id": "j0CInLijKDTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"nsys\" 2>/dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGaF4mRIL2Vo",
        "outputId": "a2057e48-9c29-4ab2-bb65-e6224332594d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/nvidia/nsight-compute/2024.2.1/host/target-linux-x64/nsys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nsys_path = \"/opt/nvidia/nsight-compute/2024.2.1/host/target-linux-x64\"\n",
        "\n",
        "os.environ['PATH'] = nsys_path + ':' + os.environ['PATH']"
      ],
      "metadata": {
        "id": "HD9FnpSXMAtM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile -t nvtx,cuda -o report_lidar --stats=true python train_pytorch.py > nsys_summary.txt 2>&1 # Save the log"
      ],
      "metadata": {
        "id": "x8DIkWSZKCv_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Profiling from a previous code run, train_pytorch from a previous script, doesn't exist anymore as I moved everything here, code left for clarity)\n",
        "\n",
        "Looking at the profile, the total time for 1 epoch is about 150 seconds with 20 seconds being GPU compute time. This means that a little under 130 seconds of total time is spent idle/on CPU.\n",
        "\n",
        "The bottleneck is almost surely the spherical projection as it is the most computationally intensive portion of the pipeline still being done on the CPU.\n",
        "\n",
        "Let's write a custom kernel for the function to make it run on the GPU.\n"
      ],
      "metadata": {
        "id": "1I1VEY-PGPQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA Kernel\n",
        "---\n"
      ],
      "metadata": {
        "id": "HFLSq-6O0TKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using pybind11 to wrap the C++ function in python"
      ],
      "metadata": {
        "id": "qQ4Itl8qHuIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybind11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W69tSml0U9M",
        "outputId": "3ab98e0e-12bf-4032-fc62-a8c609095a8b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybind11\n",
            "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/293.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m286.7/293.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoLab doesn't support C++ syntax so I will write it outside and paste it here."
      ],
      "metadata": {
        "id": "jZ9W0x0uHzl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile projection_kernel.cu\n",
        "#include <pybind11/pybind11.h>\n",
        "#include <pybind11/numpy.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cmath> // For M_PI, sin, etc.\n",
        "#include <cfloat> // For FLT_MAX\n",
        "\n",
        "namespace py = pybind11;\n",
        "\n",
        "// I'm getting an error about the atomicMin function on floats so\n",
        "// this function uses a Compare-and-Swap loop to safely\n",
        "// perform a minimum operation on floating-point numbers.\n",
        "\n",
        "__device__ static float atomicMinFloat(float* address, float val) {\n",
        "    int* address_as_int = (int*)address;\n",
        "    int old = *address_as_int;\n",
        "    int assumed;\n",
        "    do {\n",
        "        assumed = old;\n",
        "        if (__int_as_float(assumed) < val) {\n",
        "            break;\n",
        "        }\n",
        "        old = atomicCAS(address_as_int, assumed, __float_as_int(val));\n",
        "    } while (assumed != old);\n",
        "    return __int_as_float(old);\n",
        "}\n",
        "\n",
        "__global__ void projection_kernel(const float* points, int num_points,\n",
        "                                float* range_buffer, int* index_buffer,\n",
        "                                int image_height, int image_width,\n",
        "                                float fov_up, float fov_down) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (index < num_points) {\n",
        "        float x = points[index * 3 + 0];\n",
        "        float y = points[index * 3 + 1];\n",
        "        float z = points[index * 3 + 2];\n",
        "\n",
        "        float r = sqrtf(x * x + y * y + z * z);\n",
        "        if (r < 1e-6) return;\n",
        "\n",
        "        float theta = atan2f(y, x);\n",
        "        float phi = asinf(z / r);\n",
        "\n",
        "        float vertical_fov = fov_up - fov_down;\n",
        "        float u = (image_width - 1) * (1.0f - (theta + M_PI) / (2.0f * M_PI));\n",
        "        float v = (image_height - 1) * (1.0f - (phi - fov_down) / vertical_fov);\n",
        "\n",
        "        int u_int = static_cast<int>(roundf(u));\n",
        "        int v_int = static_cast<int>(roundf(v));\n",
        "        if (u_int < 0 || u_int >= image_width || v_int < 0 || v_int >= image_height) return;\n",
        "\n",
        "        int pixel_index = v_int * image_width + u_int;\n",
        "\n",
        "        float old_range = atomicMinFloat(&range_buffer[pixel_index], r);\n",
        "\n",
        "        if (r < old_range) {\n",
        "            index_buffer[pixel_index] = index;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void initialize_buffers_kernel(float* range_buffer, int* index_buffer, int num_elements) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (index < num_elements) {\n",
        "        range_buffer[index] = FLT_MAX;\n",
        "        index_buffer[index] = -1;\n",
        "    }\n",
        "}\n",
        "\n",
        "py::array_t<int> spherical_projection_cuda(py::array_t<float, py::array::c_style | py::array::forcecast> points_py,\n",
        "                                           int image_height, int image_width,\n",
        "                                           float fov_up_deg, float fov_down_deg) {\n",
        "    py::buffer_info points_buf = points_py.request();\n",
        "    const float *points_ptr = (const float *)points_buf.ptr;\n",
        "    int num_points = points_buf.shape[0];\n",
        "\n",
        "    float *d_points, *d_range_buffer;\n",
        "    int *d_index_buffer;\n",
        "    size_t num_pixels = image_height * image_width;\n",
        "\n",
        "    cudaMalloc(&d_points, num_points * 3 * sizeof(float));\n",
        "    cudaMalloc(&d_range_buffer, num_pixels * sizeof(float));\n",
        "    cudaMalloc(&d_index_buffer, num_pixels * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_points, points_ptr, num_points * 3 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threads_per_block_init = 256;\n",
        "    int blocks_per_grid_init = (num_pixels + threads_per_block_init - 1) / threads_per_block_init;\n",
        "    initialize_buffers_kernel<<<blocks_per_grid_init, threads_per_block_init>>>(d_range_buffer, d_index_buffer, num_pixels);\n",
        "\n",
        "    int threads_per_block_proj = 256;\n",
        "    int blocks_per_grid_proj = (num_points + threads_per_block_proj - 1) / threads_per_block_proj;\n",
        "\n",
        "    float fov_up_rad = fov_up_deg * M_PI / 180.0f;\n",
        "    float fov_down_rad = fov_down_deg * M_PI / 180.0f;\n",
        "\n",
        "    projection_kernel<<<blocks_per_grid_proj, threads_per_block_proj>>>(d_points, num_points, d_range_buffer, d_index_buffer, image_height, image_width, fov_up_rad, fov_down_rad);\n",
        "\n",
        "    auto result_py = py::array_t<int>(num_pixels);\n",
        "    py::buffer_info result_buf = result_py.request();\n",
        "    int *result_ptr = (int *)result_buf.ptr;\n",
        "    cudaMemcpy(result_ptr, d_index_buffer, num_pixels * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(d_points);\n",
        "    cudaFree(d_range_buffer);\n",
        "    cudaFree(d_index_buffer);\n",
        "\n",
        "    result_py.resize({image_height, image_width});\n",
        "    return result_py;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(projection_kernel, m) {\n",
        "    m.def(\"spherical_projection\", &spherical_projection_cuda, \"CUDA-accelerated Spherical Projection\");\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEgZGj12MkCN",
        "outputId": "7df5913e-af80-4a52-a523-4e3150f550ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing projection_kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -shared -std=c++20 -Xcompiler -fPIC `python3 -m pybind11 --includes` projection_kernel.cu -o projection_kernel`python3-config --extension-suffix`"
      ],
      "metadata": {
        "id": "PfjVw2FXM2wq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "id": "jBVUwY4l4M5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0a743c-8b9e-46e9-814f-2a7a94f05bcf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2132\n",
            "-rw-r--r-- 1 root root   17551 Sep 21 20:33 cnn.py\n",
            "drwx------ 5 root root    4096 Sep 21 20:33 drive\n",
            "-rw-r--r-- 1 root root    7999 Sep 21 20:38 nsys_summary.txt\n",
            "-rwxr-xr-x 1 root root 1210592 Sep 21 20:38 projection_kernel.cpython-310-x86_64-linux-gnu.so\n",
            "-rw-r--r-- 1 root root    4371 Sep 21 20:38 projection_kernel.cu\n",
            "drwxr-xr-x 2 root root    4096 Sep 21 20:33 __pycache__\n",
            "-rw-rw-r-- 1 root root  268147 Sep 21 20:38 report_lidar.nsys-rep\n",
            "-rw-r--r-- 1 root root  651264 Sep 21 20:38 report_lidar.sqlite\n",
            "drwxr-xr-x 1 root root    4096 Sep 16 13:40 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking\n",
        "---\n",
        "\n",
        "Alright, we have all of the pieces necessary for the final benchmarking.\n",
        "\n",
        "Let's see how much we sped up the pipeline by."
      ],
      "metadata": {
        "id": "GftJ-QCsrVM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util, sys\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"projection_kernel\", \"./projection_kernel.cpython-310-x86_64-linux-gnu.so\")\n",
        "mod = importlib.util.module_from_spec(spec); spec.loader.exec_module(mod)\n",
        "\n",
        "projection_kernel = mod"
      ],
      "metadata": {
        "id": "3kIFvTk4wobe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This import is included here as CoLab doesn't seem to see the compiled C++ function."
      ],
      "metadata": {
        "id": "wLzZ_OQGISne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ZIP_PATH = '/content/drive/MyDrive/data_odometry_voxels_all.zip'\n",
        "SEQUENCES_TO_TEST = ['00', '01', '02', '03', '04', '05', '06', '07'] # Full training set\n",
        "\n",
        "with zipfile.ZipFile(DATASET_ZIP_PATH, 'r') as archive:\n",
        "    all_files = archive.namelist()\n",
        "    test_files = sorted([f for f in all_files if len(f.split('/')) > 2 and f.split(\"/\")[2] in SEQUENCES_TO_TEST and f.startswith(f'dataset/sequences/{f.split(\"/\")[2]}/voxels/') and f.endswith('.bin')])\n",
        "print(f\"Found {len(test_files)} point cloud files to benchmark.\")\n",
        "\n",
        "print(\"\\n--- Benchmarking Performance Over Full Dataset ---\")\n",
        "IMAGE_HEIGHT = 64\n",
        "IMAGE_WIDTH = 1024\n",
        "FOV_UP_DEG = 25.0\n",
        "FOV_DOWN_DEG = -25.0\n",
        "\n",
        "# Time the Python/NumPy version over all files\n",
        "print(\"Running Python (CPU) benchmark\")\n",
        "start_time_py = time.perf_counter()\n",
        "with zipfile.ZipFile(DATASET_ZIP_PATH, 'r') as archive:\n",
        "    for file_path in test_files:\n",
        "        pc_bytes = archive.read(file_path)\n",
        "        point_cloud = np.frombuffer(pc_bytes, dtype=np.float32).reshape(-1, 4)\n",
        "        points_xyz = point_cloud[:, :3]\n",
        "        spherical_projection(points_xyz, np.zeros(points_xyz.shape[0]), IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "end_time_py = time.perf_counter()\n",
        "total_time_py = end_time_py - start_time_py\n",
        "print(f\"Python (CPU) total time: {total_time_py:.2f} seconds\")\n",
        "\n",
        "# Time the C++/CUDA version over all files\n",
        "print(\"\\nRunning CUDA (GPU) benchmark\")\n",
        "start_time_cuda = time.perf_counter()\n",
        "with zipfile.ZipFile(DATASET_ZIP_PATH, 'r') as archive:\n",
        "    for file_path in test_files:\n",
        "        pc_bytes = archive.read(file_path)\n",
        "        point_cloud = np.frombuffer(pc_bytes, dtype=np.float32).reshape(-1, 4)\n",
        "        points_xyz = point_cloud[:, :3].copy()\n",
        "        projection_kernel.spherical_projection(points_xyz, IMAGE_HEIGHT, IMAGE_WIDTH, FOV_UP_DEG, FOV_DOWN_DEG)\n",
        "end_time_cuda = time.perf_counter()\n",
        "total_time_cuda = end_time_cuda - start_time_cuda\n",
        "print(f\"CUDA (GPU) total time:   {total_time_cuda:.2f} seconds\")\n",
        "\n",
        "speedup = total_time_py / total_time_cuda\n",
        "avg_time_py_ms = (total_time_py / len(test_files)) * 1000\n",
        "avg_time_cuda_ms = (total_time_cuda / len(test_files)) * 1000\n",
        "\n",
        "print(\"\\n--- Final Result ---\")\n",
        "print(f\"Average time per file (Python): {avg_time_py_ms:.4f} ms\")\n",
        "print(f\"Average time per file (CUDA):   {avg_time_cuda_ms:.4f} ms\")\n",
        "print(f\"The custom CUDA kernel achieved a {speedup:.2f}x speedup over the full dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8znn7E2rWdO",
        "outputId": "597cc3f2-6afa-463b-d52d-560268fc41d5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16338 point cloud files to benchmark.\n",
            "\n",
            "--- Benchmarking Performance Over Full Dataset ---\n",
            "Running Python (CPU) benchmark\n",
            "Python (CPU) total time: 187.42 seconds\n",
            "\n",
            "Running CUDA (GPU) benchmark\n",
            "CUDA (GPU) total time:   20.38 seconds\n",
            "\n",
            "--- Final Result ---\n",
            "Average time per file (Python): 11.4714 ms\n",
            "Average time per file (CUDA):   1.2473 ms\n",
            "The custom CUDA kernel achieved a 9.20x speedup over the full dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This timing also includes the data loading step, which is insignificant\n",
        "\n",
        "In terms of frames per second (FPS), we can assume each file to be a LiDAR sweep (frame) and we can calculate the equivalent FPS metric.\n",
        "\n",
        "CPU: 11.4714 ms → FPS ≈ 1000 / 11.4714 ms ≈ 87.1 FPS\n",
        "\n",
        "CUDA Accelerated: 1.2473 ms → FPS ≈ 1000 / 1.2473 ≈ 801.7 FPS\n",
        "\n",
        "This far exceeds the typical throughput needed for LiDAR sensors, and leaves plenty of room to increase the data resolution and frequency, allowing for more accurate models.\n"
      ],
      "metadata": {
        "id": "WetNg4lkJwRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like our hunch about the spherical projection was correct.\n",
        "\n",
        "We achieved a significant speedup using a custom CUDA kernel on the only CPU bound computational task left, the model was sped up using PyTorch and the dataloading computational load is negligible."
      ],
      "metadata": {
        "id": "JDrhMdMLJQ3K"
      }
    }
  ]
}